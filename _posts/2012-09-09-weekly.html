---
layout: default
title: 周报 
---
<p id="fortitle">周报<p>
<h4><a href="#">36</a></h4>
<pre class="preclass">
一：本周完成工作
 1.1 对通讯的进行进一步的测试
   1.1.1 由于之前的上下联通还没有全部完成，所作的通讯压力测试数据有所偏差，
         36周对压力测试进行了比较全面的测试。
   1.1.2 经过计算，128DC在混合模式下，每秒的通讯数据为1M左右，
         所以通讯数据交换机压力等在实际场合的性能瓶颈理论上不存在
         在实际的测试中，用一台单独的机器跑作为DC模拟器，另外一台机器跑上位管理
         软件，与模拟DC与管理软件在一台机器上跑的性能基本无区别。
   1.1.3 在V1.2版本的管理软件上TCP通讯采取的是TCPThreadingServer，每个链接启动一个线程，
         在V2，0的版本中，开始也采取了多线程的方式，在混合模式下性能很差，在16模拟DC情况下
         即开始timeout异常，而CPU只占用60%左右，经过查资料和测试，多线程不能充分利用多核处理器
         改为多进程后，性能明显提升，也可以充分的利用多核处理器，模拟DC可以链接到32个。
   1.1.4 在测试多进程和多线程时候，得到下面一组数据
	1：MODE = 3 混合模式，单进程处理时间 ----- 4秒/100次
	2：多线程，改善性能空间很小
	4：多进程下，20DC所用时间与单个DC多用时间差不多
	5：64DC 1000次 最慢0天0小时4分17秒834毫秒 ！0.258/次 
	6：32DC 1000次 最慢0天0小时2分8秒640毫秒  ！0.129/次 
	7：48DC 1000次 最慢0天0小时3分13秒264毫秒 ！0.193/次 
	通过上面数据，理论上多进程可以跑到48DC，实际情况是32DC比较稳定，
	在实验过程中，多线程处理会出现时间的延时，可能是网络传输的数据处理不过来会出现数据缓存
  
        说明：以上数据是在无通讯环境下进行的，数据是通过都文件的方式放如全局变量中，
             测试时间基本为数据接受后，处理的数据所用的时间

   1.1.5 通过上述数据分析，性能的优化空间有两种，
            1； 通讯方式 现在的多进程和多线程都没有满足足够的DC容量，加入其他的方式或是混合一些方式也许是
                解决的办法
            2： 改善占用时间的代码 经过profile的测试，保存wav日志，占用时间的10%左右，
                unpack占用10% 左右，后台的一些数据库操作占用20%左右（当报警时保存报警信息）。
   1.1.6 测试的机器硬软件配置简要：
         处理器：AMD 四核 800MH HZ
         内存：3750M
         系统： deepLinux 内核：3.2.29
   1.1.7 得出的现在的压力通讯数据
         混合模式下，在报警较多的情况下（每个PA每三秒一次报警），可以正常不报错跑32DC
         预处理模式下，可以不报错跑64DC

 1.2 对模拟DC改善，可以为调试提供不过通讯来提供数据数据和专跑DC模拟器的Linux虚拟机
     1.2.1 由于windows机器之前没有提供DC模拟器的支持，现装了一个VBOX的虚拟机，里面专跑DC模拟器
           虚拟机的软件已经装好，启动之后即可配置跑起来模拟DC，并在170的机器上FTP提供VBOX的软件
           和打包虚拟机的下载
     1.2.2 针对调试提供了不通过网络的数据TCP波形数据的提供模拟接口，实现的方式是读取文件，
           提供单个DC的调试
 
 1.3 对通讯部分的代码BUG修改，
     对开发过程中发现的BUG进行修改，增加在UDP通讯中，下位机报错的容错处理。

二、遇到有问题及解决方法：
1. 问题：无
2. 需领导协助：无


三：下周计划
 3.1 继续测试和改进通讯性能
   3.1.1  找出代码中可以改善的地方，比如建立数据库处理的缓存层
   3.1.2  实验其他的通讯方式，比如进程线程混合模式，协程或是寻找一下其他的轻量进程线程实现方法
   3.1.3  深入的研究一下Python的协作模式，和内存缓存机制
 3.2 跟随进度改善代码，UDP部分的log日志部分，TCP部分的log部分（log过多也会影响性能）
 3.3 根据需求，对DC模拟器提供更加方便友好的支持
</pre>
